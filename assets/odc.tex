\section{Wydajne jednowymiarowe grupowanie}
\subsection{Grupowanie jednowymiarowych danych a wydajność SUBCLU}
Algorytm SUBCLU wymaga wyznaczenia grup w każdej jednowymiarowej podprzestrzeni grupowanego zbioru danych. Grupy wyznaczane są za pomocą algorytmu DBSCAN ze stałym promieniem sąsiedztwa $\varepsilon$ . Projekcja wektora na podprzestrzeń ma zawsze długość mniejszą niż lub równą długości oryginalnego wektora, stąd im mniejsza wymiarowość podprzestrzeni, w której przeprowadzane jest grupowanie za pomocą algorytmu DBSCAN, tym większy stosunek $\varepsilon$ do odległości między punktami zbioru danych. Duży stosunek $\varepsilon$ do odległości między punktami powoduje, że do grup włączana jest większa liczba punktów niż w wysokowymiarowych podprzestrzeniach. Ze specyfikacji algorytmu DBSCAN wynika, że dla każdego punktu, który należy do grupy, musi być wyznaczone $ \varepsilon $-otoczenie. W związku z tym w niskowymiarowych podprzestrzeniach grupowanie wymaga wyznaczenia $ \varepsilon $-otoczenia dla większej liczby punktów niż w wysokowymiarowych podprzestrzeniach i stąd algorytm DBSCAN działa niewydajnie w niskowymiarowych podprzestrzeniach. W konsekwencji grupowanie \mbox{w niskowymiarowych}, a w szczególności jednowymiarowych podprzestrzeniach jest czasochłonnym elementem algorytmu SUBCLU.\par

Wpływ efektywności wyznaczania grup w jednowymiarowych podprzestrzeniach na całkowitą efektywność algorytmu SUBCLU można zdefiniować jako stosunek sumy czasów grupowania wektorów z użyciem DBSCAN w poszczególnych podprzestrzeniach jednowymiarowych do całkowitego czasu grupowania \mbox{z użyciem} algorytmu SUBCLU. Dalej ten stosunek będzie oznaczany $ \phi $. Wartość $ \phi $ zależy między innymi od stosunku liczby podprzestrzeni jednowymiarowych, w których przeprowadzane jest grupowanie za pomocą algorytmu DBSCAN do liczby wszystkich podprzestrzeni, w których przeprowadzone jest grupowanie za pomocą algorytmu DBSCAN. Dalej ten stosunek będzie oznaczany $ \gamma $.\par

Liczba podprzestrzeni, w których jest przeprowadzane grupowanie za pomocą algorytmu DBSCAN, zależy od wymiarowości zbioru danych. Całkowita liczba podprzestrzeni $ n $-wymiarowej przestrzeni wyraża się równaniem:
\begin{equation}
	N = \displaystyle\sum_{i=0}^{n} \binom{n}{i} = 2^n
\end{equation}
Liczbę podprzestrzeni jedno-wymiarowych wyrazić można równaniem:
\begin{equation}
	N_1 = \binom{n}{1} = n
\end{equation}
Stosunek liczby podprzestrzeni jedno-wymiarowych do liczby wszystkich podprzestrzeni wynosi:
\begin{equation}
	\label{eq:odc:subspaces-to-subspaces-1d-count-ratio}
	\frac{N_1}{N} = \frac{n}{2^n}
\end{equation}
Stosunek otrzymany w  \myhyperref{eq:odc:subspaces-to-subspaces-1d-count-ratio}{równaniu} nie jest równoważny $ \gamma $. Algorytm SUBCLU nie dokonuje grupowania w podprzestrzeniach, dla których istnieje pewność, że nie zostanie utworzona żadna grupa. Dodatkowo nie ma sensu wykonywać grupowania dla zerowymiarowej podprzestrzeni. Grupowanie w przestrzeniach jednowymiarowych w algorytmie SUBCLU jest zawsze wykonywane, a niezależnie od wymiarowości zbioru danych mogą nie istnieć grupy dla podprzestrzeni więcej niż jednowymiarowych, dlatego górne ograniczenie $ \gamma $ wynosi 1. Wartość $ \gamma $ może zostać wyrażona zależnością:
\begin{equation}
	\label{eq:odc:subclu_1d_nd_subspaces_count_ratio}
	1 \ge \gamma \ge \frac{N_1}{N-N_0} = \frac{n}{2^n-1}
\end{equation}
Wraz ze wzrostem wymiarowości wykładniczo rośnie liczba podprzestrzeni i maleje wartość $ \frac{N_1}{N-N_0} $. Na tej podstawie można spodziewać się spadku $ \gamma $ i w konsekwencji spadku $ \phi $, czyli wpływu grupowania w podprzestrzeniach jednowymiarowych na wydajność SUBCLU. Stosunek $ \gamma $ uzyskany dla wywołań SUBCLU na $ n $-wymiarowych losowo wygenerowanych danych przedstawia \myhyperref{fig:odc:subclu-gamma-of-n}{rysunek}.

\input{assets/figures/subclu-gamma-of-n}

Zgodnie z oczekiwaniami $ \gamma $ maleje ze wzrostem wymiarowości przestrzeni. Nie powinno być również zaskoczeniem, że dobór większej wartości $ \mu $ wpływa na wzrost $ \gamma $. Parametr $ \mu $ odpowiada najmniejszej liczbie punktów, która musi znaleźć się w $ \varepsilon $-otoczeniu punktu, aby to $ \varepsilon $-otoczenie można było przypisać do grupy. Zwiększenie $ \mu $ powoduje, że mniej punktów zostaje przypisanych do grup i w konsekwencji grupowanie przeprowadzane jest w mniejszej liczbie wielowymiarowych podprzestrzeni. Taki sam efekt dałoby zmniejszenie $ \varepsilon $.\par
Zachowanie się wartości $ \gamma $ w zależności od wymiarowości przestrzeni mówi wiele o spodziewanych wartościach $ \phi $, ale nie jest jedynym czynnikiem, który je warunkuje. 

\input{assets/figures/subclu-phi-gamma-of-epsilon-mi}

Na \myhyperref{fig:odc:subclu-phi-gamma-of-epsilon}{rysunku} widać, że $ \phi $ maleje ze wzrostem $ \varepsilon $. Spadkowi wartości $ \phi $ towarzyszy wzrost liczności grup w $ n $-wymiarowych podprzestrzeniach. W miarę jak spada wartość $ \gamma $ i rosną liczności grup znajdowanych w podprzestrzeniach, rośnie też złożoność całego grupowania, przez co grupowanie jednowymiarowych podprzestrzeni staje się mniej znaczące wydajnościowo w stosunku do reszty podprzestrzeni.\par

\myhyperref{fig:odc:subclu-phi-gamma-of-mi}{Rysunek} przedstawia zależność $ \phi $ od wartości $ \mu $. Jak można przewidywać $ \phi $ rośnie wraz ze wzrostem $ \mu $. Na wykresie można wyróżnić dwa szybkie wzrosty $ \phi $. Następują one w momencie kiedy $ \mu $ przekracza wartość dla $ n $-wymiarowych podprzestrzeni, dla której punkty zaczynają być niekwalifikowane do grup w tych podprzestrzeniach na podstawie kryterium $ \mu $. SUBCLU stosuje optymalizację, która pozwala poddawać grupowaniu tylko punkty istniejące w grupach $ n-1 $ wymiarowych podprzestrzeni, stąd wynika przesunięcie $ \gamma $ względem $ \phi $. Gdyby nie ta optymalizacja skoki wartości $ \phi $ pokrywałyby się ze skokami wartości $ \gamma $.

Jak pokazano wcześniej, wartości $ \phi $ zależą w nietrywialny sposób od rozkładu danych oraz parametrów algorytmu. W uproszczeniu można przyjąć, że wydajność grupowania w podprzestrzeniach jednowymiarowych ma największe znaczenie dla danych o niewielkiej liczbie wymiarów (realnie kilkanaście) lub dużych wartościach $ \mu $. Zależność od $ \varepsilon $ nie jest prosta. Wartości $ \phi $ uzyskane dla wywołań SUBCLU na $ n $-wymiarowych, losowo wygenerowanych danych przedstawia \myhyperref{fig:odc:subclu-phi-of-n}{rysunek}.

\input{assets/figures/subclu-phi-of-n}

Wyniki pokazują, że dla przestrzeni kilku, kilkunastowymiarowych czas grupowania w jednowymiarowych podprzestrzeniach może sięgać nawet kilkunastu procent całkowitego czasu grupowania. Optymalizacja grupowania w jednowymiarowej przestrzeni może zatem przynieść wymierny zysk wydajnościowy dla algorytmu SUBCLU. 

\subsection{Wydajny algorytm grupowania jednowymiarowych danych}
Na początku warto przemyśleć, skąd bierze się problem z wydajnością grupowania jednowymiarowych danych za pomocą typowej implementacji DBSCAN. Zagadnienie przedstawię na przykładzie DBSCAN z wykorzystaniem metody nierówności trójkąta. \myhyperref{odc:tidbscan:linear}{Rysunek} przedstawia poszukiwanie sąsiedztwa pewnego punktu przy pomocy metody nierówności trójkąta. Istotą tej metody jest ograniczenie zbioru przeszukiwanych danych do punktów spełniających warunek $ \abs{d(v, r) - d(w, r)} \le \varepsilon $. Ograniczenie to jest możliwe dzięki wcześniejszemu posortowaniu danych po odległości od punktu referencyjnego. Gdy dane są uporządkowane, punkty spełniające warunek ograniczenia znajdują się w sąsiedztwie punktu, dla którego poszukiwane jest $ \varepsilon $-otoczenie. Jak widać na \myhyperref{odc:tidbscan:linear}{rysunku}, nie wszystkie punkty w ograniczonym zbiorze należą do poszukiwanego $ \varepsilon $-otoczenia, co sprawia, że dla każdego punktu z ograniczonego zbioru trzeba sprawdzić, czy należy do poszukiwanego $ \varepsilon $-otoczenia. W ten sposób ograniczamy liczbę punktów potrzebnych do przeszukania z $ |D| $ do liczby, którą można oszacować na $ \frac{\varepsilon}{\max{D}-\min{D}}|D| $. Wydajność uzyskanego tak przyśpieszenia grupowania jest proporcjonalna do $ \frac{\varepsilon}{\max{D}-\min{D}} $, stąd najlepiej sprawdza się dla małych $ \varepsilon $. Niestety, złożoność tego rozwiązania wynosi $ \mathcal{O}(n) $ i przekłada się na złożoność $ \mathcal{O}(n^2) $ grupowania, co wróży problemy wydajnościowe przy grupowaniu dużych zbiorów danych.\par

\input{assets/figures/tidbscan_1d}

Jeśli grupujemy wielowymiarowe przestrzenie, konieczne jest sprawdzanie każdego punktu ze zbioru ograniczonego za pomocą nierówności trójkąta. \mbox{W przestrzeni} jedno-wymiarowej można jednak spostrzec pewną prawidłowość. Jeśli punkt referencyjny zostanie wyniesiony na skraj zbioru danych, to ograniczony obszar zawsze będzie zawierał tylko punkty należące do poszukiwanego $ \varepsilon $-otoczenia (\myhyperref{odc:tidbscan:log}{rysunek}). Jest tak, ponieważ po przekształceniu punktów do dystansu od punktu referencyjnego nie zmieniają się między nimi odległości. Wiedząc, że obszar ograniczony nierównością trójkąta jest równoważny $ \varepsilon $-otoczeniu poszukiwanego punktu, możemy usprawnić wyznaczanie tego $ \varepsilon $-otoczenia. Nie jest już konieczne sprawdzanie każdego z punktów z ograniczonego obszaru, wystarczy wyznaczyć granice tego obszaru. Można to zrobić w czasie $ \mathcal{O}(\log{}n) $ za pomocą zmodyfikowanego przeszukiwania binarnego\cite{binaryboundarysearch}. Jest to usprawnienie, które co prawda daje nam lepszą pesymistyczną złożoność znajdowania $ \varepsilon $-otoczenia, ale procedura DBSCAN wciąż sprawdza dla każdego punktu znalezionego $ \varepsilon $-otoczenia, czy należy już do jakiejś grupy, co przekłada się na taką samą złożoność grupowania jak wcześniej $ \mathcal{O}(n^2) $.\par

\input{assets/figures/dbscan_groups_1d}

Jeśli przyjrzymy się dowolnym grupom w jednowymiarowej przestrzeni\linebreak (\myhyperref{odc:dbscan:groups}{rysunek}), możemy zauważyć, że jeżeli punkty są posortowane po współrzędnej, to grupy składają się tylko z sąsiadujących na osi punktów. Między dwoma punktami należącymi do grupy $ c $ mogą istnieć tylko punkty należące do $ c $. Dowód jest prosty, skoro dwa punkty należą do tej samej grupy, to muszą być gęstościowo połączone. Jeśli $ v_1 $ jest gęstościowo połączony \mbox{z $ v_2 $}, to musi być bezpośrednio gęstościowo połączony z punktem rdzeniowym znajdującym się między $ v_1 $ i $ v_2 $. To samo aplikuje się do punktu rdzeniowego, który też należy do tej samej grupy, więc musi być gęstościowo połączony z $ v_2 $. \mbox{W ten} sposób rekurencyjnie dochodzimy do wniosku, że cały obszar między $ v_1 $ oraz $ v_2 $ musi być \mbox{w $ \varepsilon $-otoczeniu } jakiegoś punktu rdzeniowego należącego do grupy $ c $, stąd wszystkie punkty między $ v_1 $ i $ v_2 $ należą do $ c $. Warto też zwrócić uwagę, że nie każdy punkt między $ v_1 $ i $ v_2 $ musi być punktem rdzeniowym, przypadek, w którym punkt brzegowy nie znajduje się na brzegu grupy jest pokazany na \myhyperref{odc:dbscan:edge-points}{rysunku}.\par
Okazuje się, że dzięki poprzednim spostrzeżeniom można skonstruować algorytm, który pozwala na bardzo wydajne grupowanie w jednowymiarowych przestrzeniach (\myhyperref{odc:odc}{algorytm}). Proponowany algorytm wyznacza grupy zgodne z definicją stosowaną w DBSCAN. Otrzymane grupy mogą się różnić co do punktów brzegowych w stosunku do rezultatów otrzymanych za pomocą DBSCAN. Najpierw należy posortować dane po współrzędnej. Dalej algorytm działa na zasadzie przesuwania się od początku do końca posortowanej listy punktów, po drodze odpowiednio oznaczając grupy. Dzięki wykorzystaniu dodatkowych dwóch liczników, lewego oraz prawego, $ \varepsilon $-otoczenie oznaczanego przez algorytm punktu jest natychmiast znane.\par

\input{assets/algorithms/odc}

Najbardziej czasochłonną operacją algorytmu jest sortowanie, dlatego złożoność wynosi $ \mathcal{O}(n\log{}n) $. Jeśli dane są już posortowane, to algorytm wymaga $ n $ wykonań pętli w linii 7. Wewnątrz znajdują się również pętle \mbox{w liniach 9} oraz 10. Pętla w linii 9 jest sterowana parametrem $ l $, czyli lewą granicą $ \varepsilon $-otoczenia punktu. Wartość $ l $ nie może przekroczyć pozycji aktualnego punktu $ v $, dlatego pętla wykona się podczas trwania całego algorytmu co najwyżej $ n $ razy. Pętla w linii 10, sterowana parametrem $ r $, czyli prawą granicą $ \varepsilon $-otoczenia, nie może wyjść poza granicę listy punktów, stąd również wykona się co najwyżej $ n $ razy podczas trwania algorytmu. W linii 18 również znajduje się pętla. Działanie tej pętli może wydawać się nieoczywiste. Jej zadaniem jest przypisywanie punktom identyfikatora grupy. Przypisywanie zaczyna się od prawej granicy $ \varepsilon $-otoczenia i kończy, kiedy algorytm natrafi na punkt o identyfikatorze grupy, taki jak przypisywany albo zostanie osiągnięta lewa granica $ \varepsilon $-otoczenia. Wynika stąd, że dla danej pozycji pętla może zostać wykonana co najwyżej dwa razy. Pętla nie wykona się ani razu, kiedy punkt jest szumem, raz kiedy punktem rdzeniowym, a dwa razy dla punktu brzegowego. W ten sposób można oszacować, że pętla w linii 18 wykona się nie więcej niż $ 2n $ razy. Wykonana analiza pokazuje, że algorytm zastosowany na posortowanych danych ma złożoność liniową $ \mathcal{O}(n). $\par

\input{assets/figures/odc-vs-dbscan-ratio}

\myhyperref{odc:odc-vs-dbscan-ratio}{Rysunek} przedstawia porównanie wydajności grupowania jednowymiarowych danych przy pomocy DBSCAN z wykorzystanie metody nierówności trójkąta oraz \myhyperref{odc:odc}{algorytmu}. Jak można oczekiwać jest to funkcja niewiele wolniej rosnąca niż liniowa. Już dla stosunkowo niewielkiej liczby punktów \myhyperref{odc:odc}{algorytm} jest o dwa rzędy wielkości szybszy niż DBSCAN z wykorzystaniem metody nierówności trójkąta. Na \myhyperref{odc:odc-and-dbscan}{rysunku} można zobaczyć czasy wykonania \myhyperref{odc:odc}{algorytmu} oraz DBSCAN z wykorzystanie metody nierówności trójkąta. Kształty powstałych wykresów rzeczywiście przypominają wyznaczone złożoności, $ \mathcal{O}(n\log{}n) $, funkcja trochę szybciej rosnąca niż liniowa dla wydajnego jednowymiarowego grupowania oraz funkcja przypominająca parabolę dla DBSCAN z wykorzystaniem metody nierówności trójkąta $ \mathcal{O}(n^2) $. Podczas gdy DBSCAN z wykorzystaniem metody nierówności trójkąta pozwala na grupowanie \mbox{w s}en\-sownym czasie danych liczonych \mbox{w d}ziesiątkach tysięcy punktów, wydajne grupowanie jednowymiarowe jest w stanie poradzić sobie z danymi wielkości milionów punktów.\par

\input{assets/tables/odc-real-test}

\input{assets/figures/odc-and-dbscan-1d}

Zastosowanie wydajnego jednowymiarowego grupowania w algorytmie SUBCLU powoduje, że przedstawione wcześniej problemy związane z grupowaniem jednowymiarowych podprzestrzeni stają się nieaktualne. Ważnym spostrzeżeniem jest, że dla każdej jednowymiarowej podprzestrzeni dane muszą być posortowane przed zastosowaniem \myhyperref{odc:odc}{algorytmu}. Sortowanie jest bardziej czasochłonne niż sam \myhyperref{odc:odc}{algorytm}, ale i tak zyskujemy względem kwadratowej złożoności DBSCAN z wykorzystaniem metody nierówności trójkąta.\par
\myhyperref{odc:real-test}{Tabela} przedstawia rezultaty testów na rzeczywistych danych z zastosowaniem oraz bez \myhyperref{odc:odc}{algorytmu} do grupowania jedno-wymiarowych podprzestrzeni. Dla $ 10 $-wymiarowego zbioru danych oszczędność czasu wyniosła około $ 5\% $ niezależnie od liczności zbioru. Ze spadkiem wymiarowości danych rośnie zysk z wykorzystania \myhyperref{odc:odc}{algorytmu}, aż do około $ 46\% $ dla $ 4 $-wymiarowej próby danych.